{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "316a0779",
   "metadata": {},
   "source": [
    "# Wikipedia Notable Life Expectancies\n",
    "# [Notebook  13: Models](https://github.com/teresahanak/wikipedia-life-expectancy/blob/main/wp_life_expect_models_thanak_2022_10_14.ipynb)\n",
    "### Context\n",
    "\n",
    "The\n",
    "### Objective\n",
    "\n",
    "The\n",
    "### Data Dictionary\n",
    "- Feature: Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99245d51",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453bba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To structure code automatically\n",
    "%load_ext nb_black\n",
    "\n",
    "# To import/export sqlite databases\n",
    "# import sqlite3 as sql\n",
    "\n",
    "# To save/open python objects in pickle file\n",
    "import pickle\n",
    "\n",
    "# To help with reading, cleaning, and manipulating data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# To help with data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# To be used for missing value imputation\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# To help with model building\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    BaggingRegressor,\n",
    ")\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# To randomly split data, for cross validation, and to check model performance\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    "    mean_absolute_percentage_error,\n",
    ")\n",
    "\n",
    "# To be used for data scaling and one hot encoding\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "# To be used for tuning the model\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# To be used for hyperparameter tuning searches\n",
    "from scipy.stats import loguniform\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import expon\n",
    "\n",
    "# To be used for creating pipelines and personalizing them\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# To define maximum number of columns to be displayed in a dataframe\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# To define the maximum number of rows to be displayed in a dataframe\n",
    "pd.set_option(\"display.max_rows\", 211)\n",
    "\n",
    "# To set some dataframe visualization attributes\n",
    "pd.set_option(\"max_colwidth\", 150)\n",
    "\n",
    "# To supress scientific notations for a dataframe\n",
    "# pd.set_option(\"display.float_format\", lambda x: \"%.3f\" % x)\n",
    "\n",
    "# To supress warnings\n",
    "# import warnings\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# To set some plot visualization attributes\n",
    "sns.set_theme()\n",
    "sns.set(font_scale=1.4)\n",
    "sns.set_palette(\n",
    "    (\n",
    "        \"midnightblue\",\n",
    "        \"goldenrod\",\n",
    "        \"maroon\",\n",
    "        \"darkolivegreen\",\n",
    "        \"cadetblue\",\n",
    "        \"tab:purple\",\n",
    "        \"yellowgreen\",\n",
    "    )\n",
    ")\n",
    "# plt.rc(\"font\", size=12)\n",
    "# plt.rc(\"axes\", titlesize=15)\n",
    "# plt.rc(\"axes\", labelsize=14)\n",
    "# plt.rc(\"xtick\", labelsize=13)\n",
    "# plt.rc(\"ytick\", labelsize=13)\n",
    "# plt.rc(\"legend\", fontsize=13)\n",
    "# plt.rc(\"legend\", fontsize=14)\n",
    "# plt.rc(\"figure\", titlesize=16)\n",
    "\n",
    "# To play auditory cue when cell has executed, has warning, or has error and set chime theme\n",
    "import chime\n",
    "\n",
    "chime.theme(\"zelda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc818a82",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed005f6",
   "metadata": {},
   "source": [
    "### [Reading](https://github.com/teresahanak/wikipedia-life-expectancy/blob/main/wp_life_expect_train_preproc.csv), Sampling, and Checking Data Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca58a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset\n",
    "data = pd.read_csv(\"wp_life_expect_train_preproc.csv\")\n",
    "\n",
    "# Making a working copy\n",
    "df = data.copy()\n",
    "\n",
    "# Checking the shape\n",
    "print(f\"There are {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "\n",
    "# Checking first 2 rows of the data\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cca416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking last 2 rows of the data\n",
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6e8ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking a sample of the data\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5f29da",
   "metadata": {},
   "source": [
    "### Checking Data Types and Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf505f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking data types and null values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c459d7f8",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- With our dataset loaded, we are ready for modeling.\n",
    "- We have three variables that need typcasting from object to category, then one hot encoding just prior to modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a5c9ec",
   "metadata": {},
   "source": [
    "#### Typecasting `region`, `prior_region`, and `known_for` as Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c289d284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typcasting prior_region and region as categorical\n",
    "df[[\"prior_region\", \"region\", \"known_for\"]] = df[\n",
    "    [\"prior_region\", \"region\", \"known_for\"]\n",
    "].astype(\"category\")\n",
    "\n",
    "# Re-check info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b002957d",
   "metadata": {},
   "source": [
    "## Data Preparation for Modeling\n",
    "In contrast to building the [linear regression model](https://github.com/teresahanak/wikipedia-life-expectancy/blob/main/wp_life_expect_olsmodel_thanak_2022_10_9.ipynb), we will be tuning these models.  So, we will split the train set into train and validation sets and utilize the `test` set only to check out-of-sample performance of the champion model.  We will load and treat the test set at that point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73158160",
   "metadata": {},
   "source": [
    "### Defining Independent and Dependent Variables for Train and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2d2a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating list of predictor columns\n",
    "predictor_cols = [\n",
    "    \"num_references\",\n",
    "    \"years\",\n",
    "    \"region\",\n",
    "    \"prior_region\",\n",
    "    \"known_for\",\n",
    "]\n",
    "\n",
    "# Defining target column\n",
    "target = \"age\"\n",
    "\n",
    "# Defining independent and dependent variables\n",
    "X = df[predictor_cols]\n",
    "y = df[target]\n",
    "\n",
    "# One hot encoding of categorical predictors and typecasting all predictors as float\n",
    "X = pd.get_dummies(X, drop_first=True).astype(\"float64\")\n",
    "\n",
    "# Splitting into 70:30 train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Checking shape of train and validation sets\n",
    "print(\n",
    "    f\"There are {X_train.shape[0]} rows and {X_train.shape[1]} columns in the train set.\\n\"\n",
    ")\n",
    "print(\n",
    "    f\"There are {X_val.shape[0]} rows and {X_val.shape[1]} columns in the validation set.\\n\"\n",
    ")\n",
    "\n",
    "# Checking a sample\n",
    "X_train.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ad8673",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "#### Model Evaluation Criterion\n",
    "The predictions made by the regressors will have the following performance metrics:\n",
    "- RMSE\n",
    "- MAE\n",
    "- R$^2$\n",
    "- Ajusted R$^2$\n",
    "- MAPE\n",
    "\n",
    "#### Which Metric to Optimize?\n",
    "- For hyperparameter tuning, we will optimize R$^2$, which is the proportion of variation in the target that is explained by the predictors.  \n",
    "\n",
    "- To select the champion model, will compare Adjusted R$^2$.  It is the metric that represents the amount of variation in the target that is explained by the predictors, with a penalty for more predictors.  The number of included predictors may vary between algorithms, especially as we are building including examples of decion tree regressors.  R$^2$ will improve with the addition of predictors, even if they contribute very little to the model, whereas, the penalty in Adjusted R$^2$ offsets such an increase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97709911",
   "metadata": {},
   "source": [
    "#### Functions for Checking and Tuning Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40e3f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute adjusted R-squared\n",
    "def adj_r2_score(predictors, targets, predictions):\n",
    "    r2 = r2_score(targets, predictions)\n",
    "    n = predictors.shape[0]\n",
    "    k = predictors.shape[1]\n",
    "    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
    "\n",
    "\n",
    "# Function to compute MAPE\n",
    "def mape_score(targets, predictions):\n",
    "    return np.mean(np.abs((targets - predictions) / targets)) * 100\n",
    "\n",
    "\n",
    "# Function to compute and display different metrics to check performance of a regression model\n",
    "def model_performance_regression(model, predictors, target):\n",
    "    \"\"\"\n",
    "    Function to compute and return a dataframe of different metrics to check\n",
    "    regression model performance\n",
    "    \n",
    "    model: regressor\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    \"\"\"\n",
    "    # Predictions\n",
    "    pred = model.predict(predictors)\n",
    "\n",
    "    r2 = r2_score(target, pred)  # To compute R-squared\n",
    "    adjr2 = adj_r2_score(predictors, target, pred)  # To compute adjusted R-squared\n",
    "    rmse = np.sqrt(mean_squared_error(target, pred))  # To compute RMSE\n",
    "    mae = mean_absolute_error(target, pred)  # To compute MAE\n",
    "    mape = mape_score(target, pred)  # To compute MAPE\n",
    "\n",
    "    # Creating a dataframe of metrics\n",
    "    df_perf = pd.DataFrame(\n",
    "        {\n",
    "            \"RMSE\": rmse,\n",
    "            \"MAE\": mae,\n",
    "            \"R-squared\": r2,\n",
    "            \"Adj. R-squared\": adjr2,\n",
    "            \"MAPE\": mape,\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "\n",
    "    return df_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77c60af",
   "metadata": {},
   "source": [
    "#### Defining Scorer for Cross-validation and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960741e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type of scoring used to compare parameter combinations--maximizing Adj R-squared\n",
    "scorer = \"r2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022fd077",
   "metadata": {},
   "source": [
    "### Building the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640cfbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Creating list to store the models\n",
    "models = []\n",
    "\n",
    "# Appending models to the list\n",
    "models.append(('Dtree', DecisionTreeRegressor(random_state=42)))\n",
    "\n",
    "models.append(('Random Forest', RandomForestRegressor(random_state=42)))\n",
    "\n",
    "models.append(('Bagging Dtree', BaggingRegressor(random_state=42)))\n",
    "\n",
    "models.append(('GBM', GradientBoostingRegressor(random_state=42)))\n",
    "\n",
    "models.append(('AdaBoost Dtree', AdaBoostRegressor(random_state=42)))\n",
    "\n",
    "models.append(('XGB_gbtree', XGBRegressor(random_state=42)))\n",
    "\n",
    "models.append(('XGB_gblinear', XGBRegressor(random_state=42, booster='gblinear')))\n",
    "\n",
    "# Create empty list to store all model's names and CV scores\n",
    "names = []\n",
    "results = []\n",
    "\n",
    "# Loop through all models to get the mean cross validated score\n",
    "print(\"\\n\" \"Cross-Validation:\" \"\\n\")\n",
    "\n",
    "for name, model in models:\n",
    "    cv_result = cross_val_score(\n",
    "        estimator=model, X=X_train, y=y_train, scoring=scorer, cv=5\n",
    "    )\n",
    "    results.append(cv_result)\n",
    "    names.append(name)\n",
    "    print(f\"{name}: {cv_result.mean()}\")\n",
    "    \n",
    "print(\"\\n\" \"Validation Performance:\" \"\\n\")\n",
    "\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    scores = r2_score(y_val, model.predict(X_val))\n",
    "    print(\"{}: {}\".format(name, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b33eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting boxplots for CV scores of all models defined above\n",
    "fig = plt.figure(figsize=(20, 7))\n",
    "\n",
    "fig.suptitle(\"Algorithm Comparison for Cross-validation R-squared Score\")\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.xticks(rotation=30)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dde5477",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- We have negative R$^2$ values for four of the models.  This means they are performing worse than a model that merely equates the predicted values to the constant mean value of the target.\n",
    "- The remaining three models, *GBM*, *XGB_gbtree*, and *XGB_gblinear* are giving generalized performances on train and validation sets, with similar, albeit very low, R$^2$ scores as [*olsmodel3*](https://github.com/teresahanak/wikipedia-life-expectancy/blob/main/wp_life_expect_olsmodel_thanak_2022_10_9.ipynb) (0.087).  Before hyperparameter tuning, *GBM* is outperforming the other models, including *olsmodel3*, with both train and validation R$^2$ scores of ~0.10.\n",
    "- We will perform hyperparameter tuning on the top 3 models.  Purely as an exercise we will also keep *Random Forest* in the mix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd14ca10",
   "metadata": {},
   "source": [
    "#### Collecting Models with Best Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9819875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of top models so far\n",
    "top_models = [models[1]] + [models[3]] + models[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746087fe",
   "metadata": {},
   "source": [
    "#### Creating Dataframes to Compare Training and Validation Performance of Best Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95321ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty dictionary to hold the models\n",
    "models_to_tune = {}\n",
    "\n",
    "# For loop to add models to dictionary\n",
    "for model in top_models:\n",
    "    key = model[0]\n",
    "    value = model[1]\n",
    "    models_to_tune[key] = value\n",
    "\n",
    "# Initializing dataframes to compare performance of all models\n",
    "models_train_comp_df = pd.DataFrame()\n",
    "models_val_comp_df = pd.DataFrame()\n",
    "\n",
    "# For loop to add performance results of each top model\n",
    "for name, model in models_to_tune.items():\n",
    "    models_train_comp_df[name] = model_performance_regression(model, X_train, y_train).T\n",
    "    models_val_comp_df[name] = model_performance_regression(model, X_val, y_val).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef81cd40",
   "metadata": {},
   "source": [
    "#### Comparing Top Models Before Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef40071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing train performance\n",
    "print(f\"Training Performance:\")\n",
    "models_train_comp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c012a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing validation performance\n",
    "print(f\"Validation Performance:\")\n",
    "models_val_comp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fd802a",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- Here, we compare the performance on the whole train set to the validation set.\n",
    "- Only *GBM* and *XGB_gblinear* are giving generalized performances on the two sets.\n",
    "- These two are performing on par or slightly better than [*olsmodel3*](https://github.com/teresahanak/wikipedia-life-expectancy/blob/main/wp_life_expect_olsmodel_thanak_2022_10_9.ipynb), our linear regression model, for all metrics.\n",
    "- We will see if hyperparameter tuning improves their performance, again keeping *Rand Forest* and *XGB_gbtree* in the mix for demonstration and comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9381959",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f327ada",
   "metadata": {},
   "source": [
    "### *Random Forest Tuned*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c7c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming the model\n",
    "models_to_tune[\"Random Forest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29281ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Defining model\n",
    "Model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Parameter grid to pass in RandomizedSearchCV\n",
    "param_grid = { \n",
    "    \"n_estimators\": np.arange(100, 500), \n",
    "    \"min_samples_leaf\": [None] + np.arange(1, 10).tolist(),\n",
    "    \"max_features\": ['sqrt'], \n",
    "    \"max_samples\": uniform(loc=0.3, scale=0.5),\n",
    "    'criterion': ['squared_error'],\n",
    "    \"max_depth\": [None]\n",
    "}\n",
    "\n",
    "# Calling RandomizedSearchCV\n",
    "randomized_cv = RandomizedSearchCV(\n",
    "    estimator=Model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10,\n",
    "    n_jobs=-1,\n",
    "    scoring=scorer,\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Fitting parameters in RandomizedSearchCV\n",
    "randomized_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\n",
    "    \"Best parameters are {} with CV score={}:\".format(\n",
    "        randomized_cv.best_params_, randomized_cv.best_score_\n",
    "    )\n",
    ")\n",
    "\n",
    "# Chime notification when cell successfully executes\n",
    "chime.success()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93384a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model with best parameters\n",
    "Random_Forest_tuned = RandomForestRegressor(\n",
    "    criterion=\"squared_error\",\n",
    "    max_depth=None,\n",
    "    max_features=\"sqrt\",\n",
    "    max_samples=0.3909124836035503,\n",
    "    min_samples_leaf=4,\n",
    "    n_estimators=260,\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "Random_Forest_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1f674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating different metrics\n",
    "Random_Forest_tuned_train_perf = model_performance_regression(\n",
    "    Random_Forest_tuned, X_train, y_train\n",
    ")\n",
    "print(\"Training performance:\\n\", Random_Forest_tuned_train_perf)\n",
    "Random_Forest_tuned_val_perf = model_performance_regression(\n",
    "    Random_Forest_tuned, X_val, y_val\n",
    ")\n",
    "print(\"\\nValidation performance:\\n\", Random_Forest_tuned_val_perf)\n",
    "\n",
    "# Adding model to model comparison dataframes\n",
    "models_train_comp_df[\"Random Forest Tuned\"] = Random_Forest_tuned_train_perf.T\n",
    "models_val_comp_df[\"Random Forest Tuned\"] = Random_Forest_tuned_val_perf.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7277e211",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- Hyperparameter tuning improved performance for *Random Forest*.\n",
    "- The algorithm is still overfitting the train set, compared to the validation set.\n",
    "- Note that we had a 10% fit fail during cross-validation (\"UserWarning: One or more of the test scores are non-finite..\") indicating cross-validation had some folds for which hyperparameter combinations led to Nan values.  We are going to allow it here, and go with the results of the successful iterations.  *Random Forest* is not a likely candidate for the champion model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5911ac",
   "metadata": {},
   "source": [
    "### *GBM Tuned*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbc293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming the model\n",
    "models_to_tune[\"GBM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e2a34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Defining model\n",
    "Model = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Parameter grid to pass in RandomizedSearchCV\n",
    "param_grid = {\n",
    "    \"n_estimators\": np.arange(100, 500),\n",
    "    \"learning_rate\": loguniform(0.001, 1),\n",
    "    \"subsample\": uniform(loc=0.3, scale=0.5),\n",
    "    \"max_features\": uniform(loc=0.3, scale=0.5),\n",
    "}\n",
    "\n",
    "# Calling RandomizedSearchCV\n",
    "randomized_cv = RandomizedSearchCV(\n",
    "    estimator=Model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=100,\n",
    "    n_jobs=-1,\n",
    "    scoring=scorer,\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Fitting parameters in RandomizedSearchCV\n",
    "randomized_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\n",
    "    \"Best parameters are {} with CV score={}:\".format(\n",
    "        randomized_cv.best_params_, randomized_cv.best_score_\n",
    "    )\n",
    ")\n",
    "\n",
    "# Chime notification when cell successfully executes\n",
    "chime.success()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc36ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model with best parameters\n",
    "GBM_tuned = GradientBoostingRegressor(\n",
    "    random_state=42,\n",
    "    learning_rate=0.08171272700715591,\n",
    "    max_features=0.6630456668613307,\n",
    "    n_estimators=368,\n",
    "    subsample=0.7847684335570795,\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "GBM_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d62b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating different metrics\n",
    "GBM_tuned_train_perf = model_performance_regression(GBM_tuned, X_train, y_train)\n",
    "print(\"Training performance:\\n\", GBM_tuned_train_perf)\n",
    "GBM_tuned_val_perf = model_performance_regression(GBM_tuned, X_val, y_val)\n",
    "print(\"\\nValidation performance:\\n\", GBM_tuned_val_perf)\n",
    "\n",
    "# Adding model to model comparison dataframes\n",
    "models_train_comp_df[\"GBM Tuned\"] = GBM_tuned_train_perf.T\n",
    "models_val_comp_df[\"GBM Tuned\"] = GBM_tuned_val_perf.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858f9768",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- The performance for *GBM* is improved with hyperparameter tuning.  \n",
    "- There is a slight increase in overfitting, but the validation metrics are better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fa8124",
   "metadata": {},
   "source": [
    "### *XGB_gbtree Tuned*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780a738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming the model\n",
    "models_to_tune[\"XGB_gbtree\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fcad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Defining model\n",
    "Model = XGBRegressor(random_state=42, booster='gbtree')\n",
    "\n",
    "# Parameter grid to pass in RandomizedSearchCV\n",
    "param_grid={\n",
    "    'n_estimators': np.arange(100, 500),\n",
    "    \"learning_rate\": uniform(0.1, 0.3), # aka eta\n",
    "    'gamma': expon(), # aka min_split_loss\n",
    "    'subsample': uniform(loc=0.6, scale=0.2), # proportion of train set to randomly sample prior to growing trees\n",
    "    'max_depth': np.arange(3, 8).tolist(),\n",
    "    'colsample_bytree': uniform(loc=0.3, scale=0.5)\n",
    "}\n",
    "\n",
    "# Calling RandomizedSearchCV\n",
    "randomized_cv = RandomizedSearchCV(\n",
    "    estimator=Model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=100,\n",
    "    n_jobs=-1,\n",
    "    scoring=scorer,\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Fitting parameters in RandomizedSearchCV\n",
    "randomized_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\n",
    "    \"Best parameters are {} with CV score={}:\".format(\n",
    "        randomized_cv.best_params_, randomized_cv.best_score_\n",
    "    )\n",
    ")\n",
    "\n",
    "# Chime notification when cell successfully executes\n",
    "chime.success()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0599b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model with best parameters\n",
    "XGB_gbtree_tuned = XGBRegressor(\n",
    "    booster=\"gbtree\",\n",
    "    random_state=42,\n",
    "    colsample_bytree=0.42649508399462055,\n",
    "    gamma=1.188792356281234,\n",
    "    learning_rate=0.12263036412693079,\n",
    "    max_depth=3,\n",
    "    n_estimators=404,\n",
    "    subsample=0.7391497377969234,\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "XGB_gbtree_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b696b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating different metrics\n",
    "XGB_gbtree_tuned_train_perf = model_performance_regression(\n",
    "    XGB_gbtree_tuned, X_train, y_train\n",
    ")\n",
    "print(\"Training performance:\\n\", XGB_gbtree_tuned_train_perf)\n",
    "XGB_gbtree_tuned_val_perf = model_performance_regression(XGB_gbtree_tuned, X_val, y_val)\n",
    "print(\"\\nValidation performance:\\n\", XGB_gbtree_tuned_val_perf)\n",
    "\n",
    "# Adding model to model comparison dataframes\n",
    "models_train_comp_df[\"XGB_gbtree Tuned\"] = XGB_gbtree_tuned_train_perf.T\n",
    "models_val_comp_df[\"XGB_gbtree Tuned\"] = XGB_gbtree_tuned_val_perf.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8858fe1",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- The performance for *XGB_gbtree* is improved with hyperparameter tuning.  \n",
    "- There is a slight increase in overfitting, but the validation metrics are better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990ae030",
   "metadata": {},
   "source": [
    "### *XGB_gblinear Tuned*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f31138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming the model\n",
    "models_to_tune[\"XGB_gblinear\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f87e728",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Defining model\n",
    "Model = XGBRegressor(random_state=42, booster='gblinear')\n",
    "\n",
    "# Parameter grid to pass in RandomizedSearchCV\n",
    "param_grid={\n",
    "    'n_estimators': np.arange(100, 500),\n",
    "    'reg_lambda': loguniform(.0001, 1)\n",
    "}\n",
    "\n",
    "# Calling RandomizedSearchCV\n",
    "randomized_cv = RandomizedSearchCV(\n",
    "    estimator=Model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=100,\n",
    "    n_jobs=-1,\n",
    "    scoring=scorer,\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Fitting parameters in RandomizedSearchCV\n",
    "randomized_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\n",
    "    \"Best parameters are {} with CV score={}:\".format(\n",
    "        randomized_cv.best_params_, randomized_cv.best_score_\n",
    "    )\n",
    ")\n",
    "\n",
    "# Chime notification when cell successfully executes\n",
    "chime.success()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a984957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model with best parameters\n",
    "XGB_gblinear_tuned = XGBRegressor(\n",
    "    booster=\"gblinear\",\n",
    "    random_state=42,\n",
    "    n_estimators=439,\n",
    "    reg_lambda=0.0009206654892274761,\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "XGB_gblinear_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e40cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating different metrics\n",
    "XGB_gblinear_tuned_train_perf = model_performance_regression(\n",
    "    XGB_gblinear_tuned, X_train, y_train\n",
    ")\n",
    "print(\"Training performance:\\n\", XGB_gblinear_tuned_train_perf)\n",
    "XGB_gblinear_tuned_val_perf = model_performance_regression(\n",
    "    XGB_gblinear_tuned, X_val, y_val\n",
    ")\n",
    "print(\"\\nValidation performance:\\n\", XGB_gblinear_tuned_val_perf)\n",
    "\n",
    "# Adding model to model comparison dataframes\n",
    "models_train_comp_df[\"XGB_gblinear Tuned\"] = XGB_gblinear_tuned_train_perf.T\n",
    "models_val_comp_df[\"XGB_gblinear Tuned\"] = XGB_gblinear_tuned_val_perf.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad20774",
   "metadata": {},
   "source": [
    "## Model Performance Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bca4213",
   "metadata": {},
   "source": [
    "### Performance of Various Models Tuned and Untuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1a2170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying train performance of all models\n",
    "print(\"Train Performance Comparison:\")\n",
    "models_train_comp_df.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce69a73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying validation performance of all models\n",
    "print(\"Validation Performance Comparison:\")\n",
    "models_val_comp_df.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e548f32f",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- *GBM Tuned* has the highest R$^2$ (0.109) on the validation set, followed by *XGB_gbtree Tuned*, then *GBM*.\n",
    "- As we did not include the Decision Tree here, we can ignore Adjusted R$^2$, and just compare R$^2$.\n",
    "- Of these three models with R$^2$ scores over 10, there is some variation in overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b094e5",
   "metadata": {},
   "source": [
    "#### Comparison of Percentage of Overfitting for R$^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9174947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtracting the ratio of validation R-square/train R-square from 1\n",
    "overfit_perc = (\n",
    "    1\n",
    "    - (\n",
    "        models_val_comp_df.loc[\"R-squared\", :]\n",
    "        / models_train_comp_df.loc[\"R-squared\", :]\n",
    "    )\n",
    ") * 100\n",
    "\n",
    "print(f\"Percentage of R-square overfitting:\")\n",
    "overfit_perc.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8772710f",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- *XGB_gblinear* and *XGB_gblinear Tuned* both performed better on the validation set, than the training set, which is interesting.\n",
    "- Of the top 3 models for R$^2$, *GBM* generalized considerably better than *GBM Tuned* and *XGB_gtree Tuned*.  \n",
    "- That said, *GBM Tuned* has the highest R$^2$ score on the validation set.\n",
    "- Next we will try another modeling iteration, replacing the `known_for` feature with the original `known for` category columns.  For linear regression, we had to drop categorical columns to eliminate multicollinearity, so entries with multiple `known for` categories were grouped, into `two` and `three_to_five` classes.  We retained that approach for the above modeling iteration, but for this iteration we will allow entries to have their original multiple categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9da4ae",
   "metadata": {},
   "source": [
    "## 2nd Modeling Iteration with Original `known for` Category Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6303537",
   "metadata": {},
   "source": [
    "### Defining Independent and Dependent Variables for Train and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adcc9e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating list of predictor columns\n",
    "predictor_cols = [\n",
    "    \"num_references\",\n",
    "    \"years\",\n",
    "    \"region\",\n",
    "    \"prior_region\",\n",
    "    'sciences', \n",
    "    'social',\n",
    "    'spiritual',\n",
    "    'academia_humanities',\n",
    "    'business_farming',\n",
    "    'arts',\n",
    "    'sports',\n",
    "    'law_enf_military_operator',\n",
    "    'politics_govt_law',\n",
    "    'crime'\n",
    "]\n",
    "\n",
    "# Defining target column\n",
    "target = \"age\"\n",
    "\n",
    "# Defining independent and dependent variables\n",
    "X = df[predictor_cols]\n",
    "y = df[target]\n",
    "\n",
    "# One hot encoding of categorical predictors and typecasting all predictors as float\n",
    "X = pd.get_dummies(X, drop_first=True).astype(\"float64\")\n",
    "\n",
    "# Splitting into 70:30 train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Checking shape of train and validation sets\n",
    "print(\n",
    "    f\"There are {X_train.shape[0]} rows and {X_train.shape[1]} columns in the train set.\\n\"\n",
    ")\n",
    "print(\n",
    "    f\"There are {X_val.shape[0]} rows and {X_val.shape[1]} columns in the validation set.\\n\"\n",
    ")\n",
    "\n",
    "# Checking a sample\n",
    "X_train.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46131ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type of scoring used to compare parameter combinations--maximizing Adj R-squared\n",
    "scorer = \"r2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a9c1c0",
   "metadata": {},
   "source": [
    "### Building the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5307aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Creating list to store the models\n",
    "models = []\n",
    "\n",
    "# Appending models to the list\n",
    "models.append(('Dtree2', DecisionTreeRegressor(random_state=42)))\n",
    "\n",
    "models.append(('Random Forest2', RandomForestRegressor(random_state=42)))\n",
    "\n",
    "models.append(('Bagging Dtree2', BaggingRegressor(random_state=42)))\n",
    "\n",
    "models.append(('GBM2', GradientBoostingRegressor(random_state=42)))\n",
    "\n",
    "models.append(('AdaBoost Dtree2', AdaBoostRegressor(random_state=42)))\n",
    "\n",
    "models.append(('XGB_gbtree2', XGBRegressor(random_state=42)))\n",
    "\n",
    "models.append(('XGB_gblinear2', XGBRegressor(random_state=42, booster='gblinear')))\n",
    "\n",
    "# Create empty list to store all model's names and CV scores\n",
    "names = []\n",
    "results = []\n",
    "\n",
    "# Loop through all models to get the mean cross validated score\n",
    "print(\"\\n\" \"Cross-Validation:\" \"\\n\")\n",
    "\n",
    "for name, model in models:\n",
    "    cv_result = cross_val_score(\n",
    "        estimator=model, X=X_train, y=y_train, scoring=scorer, cv=5\n",
    "    )\n",
    "    results.append(cv_result)\n",
    "    names.append(name)\n",
    "    print(f\"{name}: {cv_result.mean()}\")\n",
    "    \n",
    "print(\"\\n\" \"Validation Performance:\" \"\\n\")\n",
    "\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    scores = r2_score(y_val, model.predict(X_val))\n",
    "    print(\"{}: {}\".format(name, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e767eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting boxplots for CV scores of all models defined above\n",
    "fig = plt.figure(figsize=(20, 7))\n",
    "\n",
    "fig.suptitle(\"Algorithm Comparison for Cross-validation R-squared Score\")\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.xticks(rotation=30)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb90bb69",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- We have negative R$^2$ values for four of the models.  This means they are performing worse than a model that merely equates the predicted values to the constant mean value of the target.\n",
    "- The remaining three models, *GBM*, *XGB_gbtree*, and *XGB_gblinear* are giving generalized performances on train and validation sets, with similar, albeit very low, R$^2$ scores as [*olsmodel3*](https://github.com/teresahanak/wikipedia-life-expectancy/blob/main/wp_life_expect_olsmodel_thanak_2022_10_9.ipynb) (0.087).  Before hyperparameter tuning, *GBM* is outperforming the other models, including *olsmodel3*, with both train and validation R$^2$ scores of ~0.10.\n",
    "- We will perform hyperparameter tuning on the top 3 models.  Purely as an exercise we will also keep *Random Forest* in the mix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fac82c8",
   "metadata": {},
   "source": [
    "#### Collecting Models with Best Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49adb130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of top models so far\n",
    "top_models = [models[1]] + [models[3]] + models[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751892b7",
   "metadata": {},
   "source": [
    "#### Adding Models Training and Validation Performance Comparison Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cc1491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty dictionary to hold the models\n",
    "models_to_tune = {}\n",
    "\n",
    "# For loop to add models to dictionary\n",
    "for model in top_models:\n",
    "    key = model[0]\n",
    "    value = model[1]\n",
    "    models_to_tune[key] = value\n",
    "\n",
    "# For loop to add performance results of each top model\n",
    "for name, model in models_to_tune.items():\n",
    "    models_train_comp_df[name] = model_performance_regression(model, X_train, y_train).T\n",
    "    models_val_comp_df[name] = model_performance_regression(model, X_val, y_val).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ea19c0",
   "metadata": {},
   "source": [
    "#### Comparing Top Models Before Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45372f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing train performance\n",
    "print(f\"Training Performance:\")\n",
    "models_train_comp_df[models_to_tune]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0a9dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing validation performance\n",
    "print(f\"Validation Performance:\")\n",
    "models_val_comp_df[models_to_tune]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77978567",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- Here, we compare the performance on the whole train set to the validation set.\n",
    "- Only *GBM* and *XGB_gblinear* are giving generalized performances on the two sets.\n",
    "- These two are performing on par or slightly better than [*olsmodel3*](https://github.com/teresahanak/wikipedia-life-expectancy/blob/main/wp_life_expect_olsmodel_thanak_2022_10_9.ipynb), our linear regression model, for all metrics.\n",
    "- We will see if hyperparameter tuning improves their performance, again keeping *Rand Forest* and *XGB_gbtree* in the mix for demonstration and comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40f3c70",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eb1634",
   "metadata": {},
   "source": [
    "### *Random Forest2 Tuned*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1ed622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming the model\n",
    "models_to_tune[\"Random Forest2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1d7057",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Defining model\n",
    "Model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Parameter grid to pass in RandomizedSearchCV\n",
    "param_grid = { \n",
    "    \"n_estimators\": np.arange(100, 500), \n",
    "    \"min_samples_leaf\": [None] + np.arange(1, 10).tolist(),\n",
    "    \"max_features\": ['sqrt'], \n",
    "    \"max_samples\": uniform(loc=0.3, scale=0.5),\n",
    "    'criterion': ['squared_error'],\n",
    "    \"max_depth\": [None]\n",
    "}\n",
    "\n",
    "# Calling RandomizedSearchCV\n",
    "randomized_cv = RandomizedSearchCV(\n",
    "    estimator=Model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10,\n",
    "    n_jobs=-1,\n",
    "    scoring=scorer,\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Fitting parameters in RandomizedSearchCV\n",
    "randomized_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\n",
    "    \"Best parameters are {} with CV score={}:\".format(\n",
    "        randomized_cv.best_params_, randomized_cv.best_score_\n",
    "    )\n",
    ")\n",
    "\n",
    "# Chime notification when cell successfully executes\n",
    "chime.success()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d18f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model with best parameters\n",
    "Random_Forest2_tuned = RandomForestRegressor(\n",
    "    criterion=\"squared_error\",\n",
    "    max_depth=None,\n",
    "    max_features=\"sqrt\",\n",
    "    max_samples=0.3909124836035503,\n",
    "    min_samples_leaf=4,\n",
    "    n_estimators=260,\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "Random_Forest2_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f61b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating different metrics\n",
    "Random_Forest2_tuned_train_perf = model_performance_regression(\n",
    "    Random_Forest2_tuned, X_train, y_train\n",
    ")\n",
    "print(\"Training performance:\\n\", Random_Forest2_tuned_train_perf)\n",
    "Random_Forest2_tuned_val_perf = model_performance_regression(\n",
    "    Random_Forest2_tuned, X_val, y_val\n",
    ")\n",
    "print(\"\\nValidation performance:\\n\", Random_Forest2_tuned_val_perf)\n",
    "\n",
    "# Adding model to model comparison dataframes\n",
    "models_train_comp_df[\"Random Forest2 Tuned\"] = Random_Forest2_tuned_train_perf.T\n",
    "models_val_comp_df[\"Random Forest2 Tuned\"] = Random_Forest2_tuned_val_perf.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693fec20",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- Hyperparameter tuning improved performance for *Random Forest*.\n",
    "- The algorithm is still overfitting the train set, compared to the validation set.\n",
    "- Note that we had a 10% fit fail during cross-validation (\"UserWarning: One or more of the test scores are non-finite..\") indicating cross-validation had some folds for which hyperparameter combinations led to Nan values.  We are going to allow it here, and go with the results of the successful iterations.  *Random Forest* is not a likely candidate for the champion model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2458c4a",
   "metadata": {},
   "source": [
    "### *GBM2 Tuned*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e9e25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming the model\n",
    "models_to_tune[\"GBM2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3183333",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Defining model\n",
    "Model = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Parameter grid to pass in RandomizedSearchCV\n",
    "param_grid = {\n",
    "    \"n_estimators\": np.arange(100, 500),\n",
    "    \"learning_rate\": loguniform(0.001, 1),\n",
    "    \"subsample\": uniform(loc=0.3, scale=0.5),\n",
    "    \"max_features\": uniform(loc=0.3, scale=0.5),\n",
    "}\n",
    "\n",
    "# Calling RandomizedSearchCV\n",
    "randomized_cv = RandomizedSearchCV(\n",
    "    estimator=Model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=100,\n",
    "    n_jobs=-1,\n",
    "    scoring=scorer,\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Fitting parameters in RandomizedSearchCV\n",
    "randomized_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\n",
    "    \"Best parameters are {} with CV score={}:\".format(\n",
    "        randomized_cv.best_params_, randomized_cv.best_score_\n",
    "    )\n",
    ")\n",
    "\n",
    "# Chime notification when cell successfully executes\n",
    "chime.success()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddbf81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model with best parameters\n",
    "GBM2_tuned = GradientBoostingRegressor(\n",
    "    random_state=42,\n",
    "    learning_rate=0.08171272700715591,\n",
    "    max_features=0.6630456668613307,\n",
    "    n_estimators=368,\n",
    "    subsample=0.7847684335570795,\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "GBM2_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ceda3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating different metrics\n",
    "GBM2_tuned_train_perf = model_performance_regression(GBM2_tuned, X_train, y_train)\n",
    "print(\"Training performance:\\n\", GBM2_tuned_train_perf)\n",
    "GBM2_tuned_val_perf = model_performance_regression(GBM2_tuned, X_val, y_val)\n",
    "print(\"\\nValidation performance:\\n\", GBM2_tuned_val_perf)\n",
    "\n",
    "# Adding model to model comparison dataframes\n",
    "models_train_comp_df[\"GBM2 Tuned\"] = GBM2_tuned_train_perf.T\n",
    "models_val_comp_df[\"GBM2 Tuned\"] = GBM2_tuned_val_perf.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29d4d5b",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- The performance for *GBM2* is improved with hyperparameter tuning.  \n",
    "- There is a slight increase in overfitting, but the validation metrics are better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1ac539",
   "metadata": {},
   "source": [
    "### *XGB_gbtree2 Tuned*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35bf78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming the model\n",
    "models_to_tune[\"XGB_gbtree2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07b547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Defining model\n",
    "Model = XGBRegressor(random_state=42, booster='gbtree')\n",
    "\n",
    "# Parameter grid to pass in RandomizedSearchCV\n",
    "param_grid={\n",
    "    'n_estimators': np.arange(100, 500),\n",
    "    \"learning_rate\": uniform(0.1, 0.3), # aka eta\n",
    "    'gamma': expon(), # aka min_split_loss\n",
    "    'subsample': uniform(loc=0.6, scale=0.2), # proportion of train set to randomly sample prior to growing trees\n",
    "    'max_depth': np.arange(3, 8).tolist(),\n",
    "    'colsample_bytree': uniform(loc=0.3, scale=0.5)\n",
    "}\n",
    "\n",
    "# Calling RandomizedSearchCV\n",
    "randomized_cv = RandomizedSearchCV(\n",
    "    estimator=Model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=100,\n",
    "    n_jobs=-1,\n",
    "    scoring=scorer,\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Fitting parameters in RandomizedSearchCV\n",
    "randomized_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\n",
    "    \"Best parameters are {} with CV score={}:\".format(\n",
    "        randomized_cv.best_params_, randomized_cv.best_score_\n",
    "    )\n",
    ")\n",
    "\n",
    "# Chime notification when cell successfully executes\n",
    "chime.success()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27fa7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model with best parameters\n",
    "XGB_gbtree2_tuned = XGBRegressor(\n",
    "    booster=\"gbtree\",\n",
    "    random_state=42,\n",
    "    colsample_bytree=0.42649508399462055,\n",
    "    gamma=1.188792356281234,\n",
    "    learning_rate=0.12263036412693079,\n",
    "    max_depth=3,\n",
    "    n_estimators=404,\n",
    "    subsample=0.7391497377969234,\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "XGB_gbtree2_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ef7d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating different metrics\n",
    "XGB_gbtree2_tuned_train_perf = model_performance_regression(\n",
    "    XGB_gbtree2_tuned, X_train, y_train\n",
    ")\n",
    "print(\"Training performance:\\n\", XGB_gbtree2_tuned_train_perf)\n",
    "XGB_gbtree2_tuned_val_perf = model_performance_regression(XGB_gbtree2_tuned, X_val, y_val)\n",
    "print(\"\\nValidation performance:\\n\", XGB_gbtree2_tuned_val_perf)\n",
    "\n",
    "# Adding model to model comparison dataframes\n",
    "models_train_comp_df[\"XGB_gbtree2 Tuned\"] = XGB_gbtree2_tuned_train_perf.T\n",
    "models_val_comp_df[\"XGB_gbtree2 Tuned\"] = XGB_gbtree2_tuned_val_perf.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d675ec",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- The performance for *XGB_gbtree2* is improved with hyperparameter tuning.  \n",
    "- There is a slight increase in overfitting, but the validation metrics are better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8837161d",
   "metadata": {},
   "source": [
    "### *XGB_gblinear2 Tuned*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534d844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming the model\n",
    "models_to_tune[\"XGB_gblinear2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b286459",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Defining model\n",
    "Model = XGBRegressor(random_state=42, booster='gblinear')\n",
    "\n",
    "# Parameter grid to pass in RandomizedSearchCV\n",
    "param_grid={\n",
    "    'n_estimators': np.arange(100, 500),\n",
    "    'reg_lambda': loguniform(.0001, 1)\n",
    "}\n",
    "\n",
    "# Calling RandomizedSearchCV\n",
    "randomized_cv = RandomizedSearchCV(\n",
    "    estimator=Model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=100,\n",
    "    n_jobs=-1,\n",
    "    scoring=scorer,\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Fitting parameters in RandomizedSearchCV\n",
    "randomized_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\n",
    "    \"Best parameters are {} with CV score={}:\".format(\n",
    "        randomized_cv.best_params_, randomized_cv.best_score_\n",
    "    )\n",
    ")\n",
    "\n",
    "# Chime notification when cell successfully executes\n",
    "chime.success()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14b2830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model with best parameters\n",
    "XGB_gblinear2_tuned = XGBRegressor(\n",
    "    booster=\"gblinear\",\n",
    "    random_state=42,\n",
    "    n_estimators=439,\n",
    "    reg_lambda=0.0009206654892274761,\n",
    ")\n",
    "\n",
    "# Fit the model on training data\n",
    "XGB_gblinear2_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10517f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating different metrics\n",
    "XGB_gblinear2_tuned_train_perf = model_performance_regression(\n",
    "    XGB_gblinear2_tuned, X_train, y_train\n",
    ")\n",
    "print(\"Training performance:\\n\", XGB_gblinear2_tuned_train_perf)\n",
    "XGB_gblinear2_tuned_val_perf = model_performance_regression(\n",
    "    XGB_gblinear2_tuned, X_val, y_val\n",
    ")\n",
    "print(\"\\nValidation performance:\\n\", XGB_gblinear2_tuned_val_perf)\n",
    "\n",
    "# Adding model to model comparison dataframes\n",
    "models_train_comp_df[\"XGB_gblinear2 Tuned\"] = XGB_gblinear2_tuned_train_perf.T\n",
    "models_val_comp_df[\"XGB_gblinear2 Tuned\"] = XGB_gblinear2_tuned_val_perf.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362a46fc",
   "metadata": {},
   "source": [
    "#### Observations:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f33d06c",
   "metadata": {},
   "source": [
    "## Model Performance Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9f050d",
   "metadata": {},
   "source": [
    "### Performance of Various Models Tuned and Untuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989d4e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying train performance of all models\n",
    "print(\"Train Performance Comparison:\")\n",
    "models_train_comp_df.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4bd712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying validation performance of all models\n",
    "print(\"Validation Performance Comparison:\")\n",
    "models_val_comp_df.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568f9c63",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- *GBM Tuned* has the highest R$^2$ (0.109) on the validation set, followed by *XGB_gbtree Tuned*, then *GBM*.\n",
    "- As we did not include the Decision Tree here, we can ignore Adjusted R$^2$, and just compare R$^2$.\n",
    "- Of these three models with R$^2$ scores over 10, there is some variation in overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da17d8ff",
   "metadata": {},
   "source": [
    "#### Comparison of Percentage of Overfitting for R$^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c354468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtracting the ratio of validation R-square/train R-square from 1\n",
    "overfit_perc = (\n",
    "    1\n",
    "    - (\n",
    "        models_val_comp_df.loc[\"R-squared\", :]\n",
    "        / models_train_comp_df.loc[\"R-squared\", :]\n",
    "    )\n",
    ") * 100\n",
    "\n",
    "print(f\"Percentage of R-square overfitting:\")\n",
    "overfit_perc.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c295f5b2",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- *XGB_gblinear* and *XGB_gblinear Tuned* both performed better on the validation set, than the training set, which is interesting.\n",
    "- Of the top 3 models for R$^2$, *GBM* generalized considerably better than *GBM Tuned* and *XGB_gtree Tuned*.  \n",
    "- That said, *GBM Tuned* has the highest R$^2$ score on the validation set.\n",
    "- Next we will try another modeling iteration, replacing the `known_for` feature with the original `known for` category columns.  For linear regression, we had to drop categorical columns to eliminate multicollinearity, so entries with multiple `known for` categories were grouped, into `two` and `three_to_five` classes.  We retained that approach for the above modeling iteration, but for this iteration we will allow entries to have their original multiple categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f41a3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad58d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc0aa63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f257dd18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9812bc77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0245e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f09c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0868e09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb26634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffa0783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05870c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab231482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d9d909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2782dde8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca70cce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df64273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0611f967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e96b74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97220d90",
   "metadata": {},
   "source": [
    "### *GBM Tuned* Performance on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39e6b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking performance of champion model on test set\n",
    "GBM_tuned_test_perf = model_performance_regression(GBM_tuned, X_test, y_test)\n",
    "print(\"Test performance:\\n\", GBM_tuned_test_perf)\n",
    "\n",
    "# Creating test and train performance df\n",
    "champion_df = pd.DataFrame()\n",
    "champion_df[\"GBM Tuned Train\"] = GBM_tuned_train_perf.T\n",
    "champion_df[\"GBM Tuned Test\"] = GBM_tuned_test_perf.T\n",
    "champion_df[\"Overfit Percentage\"] = (\n",
    "    1 - (champion_df[\"GBM Tuned Test\"] / champion_df[\"GBM Tuned Train\"])\n",
    ") * 100\n",
    "champion_df.drop(\"Adj. R-squared\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4c7283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance on train and test sets\n",
    "print(\n",
    "    f'Average overfit of the 4 metrics is {np.round(champion_df[\"Overfit Percentage\"].sum()/4, 2)}%.'\n",
    ")\n",
    "champion_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243b2aba",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- *GBM Tuned*'s performance is holding up on the unseen test data.\n",
    "- We have a model that explains 10.7% of the variation in life span of notable Wikipedia individuals, who meet inclusion criteria.\n",
    "- The model predicts life expectancy within average errors of 11.5 years and 18.8%.\n",
    "- Let us check the most important predictive features of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1226c60",
   "metadata": {},
   "source": [
    "### Feature Importance of *GBM Tuned*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9e13a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting feature importances of final model\n",
    "feature_names = X_train.columns\n",
    "importances = GBM_tuned.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\n",
    "plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d105a1",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- Before deciding on a champion model, we will try another very similar approach.\n",
    "- Instead of using the extracted feature `known_for`, that grouped entries with multiple `known for` categories, we will let the original features stand.  This approach would not have worked for the basic linear regression model, because we had to drop columns to avoid multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36c0663",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Complete\")\n",
    "\n",
    "# Chime notification when cell executes\n",
    "chime.success()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aacfcc",
   "metadata": {},
   "source": [
    "# [Proceed to Data Cleaning Part ]()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
